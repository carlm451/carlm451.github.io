<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Carl's Study Notes — Computational Physics &amp; Machine Learning</title>
<link rel="stylesheet" href="assets/style.css">
</head>
<body class="page-landing">

<header>
  <div class="container">
    <h1>Computational Physics Study Notes</h1>
    <p class="subtitle">
      Self-study course materials on topics at the intersection of physics, mathematics, and machine learning. Built for my own learning; shared in case others find them useful.
    </p>
    <p class="me">
      <span class="me-highlight">Carl</span>
      <span class="me-highlight">Data Scientist</span>
      <span class="me-highlight">Physics PhD</span>
      <a href="https://github.com/carlm451">github.com/carlm451</a>
    </p>
  </div>
</header>

<main>

  <!-- ====== ACTIVE COURSES ====== -->
  <div class="section-label">Topological & Geometric Deep Learning</div>

  <a class="card featured" href="tdl/tdl-index.html">
    <span class="live-badge">Live</span>
    <div class="tags">
      <span class="tag topology">Algebraic Topology</span>
      <span class="tag ml">Deep Learning</span>
      <span class="tag physics">Graph Theory</span>
    </div>
    <h3>A Guided Introduction to Topological Deep Learning</h3>
    <p>
      Background for reading Hajij et al., "Topological Deep Learning: Going Beyond Graph Data."
      Builds from scratch: graphs → simplicial complexes → cell complexes → combinatorial complexes → higher-order message passing → Hodge theory → spectral methods. Includes worked examples, SVG diagrams, and connections to thermal physics throughout.
    </p>
    <div class="meta"><span class="status live"></span> 8 chapters · 3 parts · Worked matrix examples · February 2026</div>
  </a>

  <!-- ====== NEURAL PDE SOLVERS ====== -->
  <div class="section-label">Neural PDE Solvers</div>

  <div class="card-grid">
    <div class="card placeholder">
      <div class="tags">
        <span class="tag ml">Neural Networks</span>
        <span class="tag physics">PDEs</span>
      </div>
      <h3>Physics-Informed Neural Networks (PINNs)</h3>
      <p>Embedding PDE constraints into loss functions — theory, training dynamics, failure modes, and when they actually work.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>

    <div class="card placeholder">
      <div class="tags">
        <span class="tag ml">Neural Operators</span>
        <span class="tag physics">Spectral Methods</span>
      </div>
      <h3>Fourier Neural Operators (FNOs)</h3>
      <p>From spectral methods to operator learning — how FNOs learn PDE solution maps in Fourier space and what they can't do.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>

    <div class="card placeholder">
      <div class="tags">
        <span class="tag ml">Deep Learning</span>
        <span class="tag math">Functional Analysis</span>
      </div>
      <h3>DeepONet &amp; Neural Operator Theory</h3>
      <p>Universal approximation for operators, branch-trunk architectures, and the mathematical foundations connecting all neural PDE solvers.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>
  </div>

  <!-- ====== MATHEMATICAL FOUNDATIONS ====== -->
  <div class="section-label">Mathematical Foundations</div>

  <div class="card-grid">
    <div class="card placeholder">
      <div class="tags">
        <span class="tag math">Analysis</span>
        <span class="tag physics">Continuum Mechanics</span>
      </div>
      <h3>PDEs for Machine Learning</h3>
      <p>The PDE theory neural solvers actually need: weak solutions, Sobolev spaces, variational formulations, and well-posedness.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>

    <div class="card placeholder">
      <div class="tags">
        <span class="tag math">Optimization</span>
        <span class="tag ml">Training Dynamics</span>
      </div>
      <h3>Optimization for Deep Learning</h3>
      <p>SGD, Adam, Muon, loss landscapes, learning rate schedules, and why neural networks train at all.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>

    <div class="card placeholder">
      <div class="tags">
        <span class="tag ml">Deep Learning</span>
        <span class="tag math">Calculus</span>
      </div>
      <h3>Autodiff &amp; Backpropagation from Scratch</h3>
      <p>The chain rule, computational graphs, reverse-mode automatic differentiation, and building a minimal autograd engine in Python — the calculus that makes deep learning work.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>
  </div>
  <div class="section-label">Computational Physics</div>

  <div class="card-grid">
    <div class="card placeholder">
      <div class="tags">
        <span class="tag physics">Statistical Mechanics</span>
        <span class="tag compute">HPC</span>
      </div>
      <h3>Molecular Dynamics Simulations</h3>
      <p>Force fields, integration schemes, thermostats, ensembles, and the bridge to machine-learned interatomic potentials.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>

    <div class="card placeholder">
      <div class="tags">
        <span class="tag physics">Electrodynamics</span>
        <span class="tag compute">Simulation</span>
      </div>
      <h3>Computational Electromagnetics</h3>
      <p>FDTD, method of moments, and how Maxwell's equations connect to the de Rham complex in topological deep learning.</p>
      <div class="meta"><span class="status planned"></span> Planned</div>
    </div>
  </div>

</main>

<footer>
  <p>
    Open-source self-study materials ·
    <a href="https://github.com/carlm451/carlm451.github.io">View source on GitHub</a>
  </p>
</footer>

</body>
</html>
